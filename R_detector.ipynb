{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-transformers"
      ],
      "metadata": {
        "id": "B5KfT8ROlPn5"
      },
      "id": "B5KfT8ROlPn5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J45hFl3H0gLr"
      },
      "id": "J45hFl3H0gLr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8188c36e",
      "metadata": {
        "id": "8188c36e"
      },
      "outputs": [],
      "source": [
        "import platform, sys, torch, numpy as np\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Machine:\", platform.machine())       # should be arm64 on Apple Silicon\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"MPS available:\", torch.backends.mps.is_available())\n",
        "print(\"NumPy:\", np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd1ab82",
      "metadata": {
        "id": "2bd1ab82"
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "def get_device():\n",
        "    return (\n",
        "        torch.device(\"cuda:0\") if config.get(\"use_gpu\", False) else torch.device(\"cpu\")\n",
        "    )\n",
        "DEVICE = get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a336584e",
      "metadata": {
        "id": "a336584e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_human_path = '/content/drive/MyDrive/train_human.npy'\n",
        "train_ai_path = '/content/drive/MyDrive/train_ai.npy'\n",
        "validation_data_path = '/content/drive/MyDrive/validation.jsonl'\n",
        "net_pt_path = '/content/drive/MyDrive/net.pt'"
      ],
      "metadata": {
        "id": "V3gp2r2U0xUr"
      },
      "id": "V3gp2r2U0xUr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a100da",
      "metadata": {
        "id": "77a100da"
      },
      "outputs": [],
      "source": [
        "HWT_ref = np.load(train_human_path)\n",
        "MGT_ref = np.load(train_ai_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HWT_ref.shape"
      ],
      "metadata": {
        "id": "YL1sPBVvP9_d"
      },
      "id": "YL1sPBVvP9_d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6264199",
      "metadata": {
        "id": "e6264199"
      },
      "outputs": [],
      "source": [
        "def load_validation_data(jsonl_path):\n",
        "    all_segments = []  # list of (n_segments, 100, 768)\n",
        "    all_labels = []    # list of int (0 or 1)\n",
        "    all_ids = []       # list of int or str\n",
        "\n",
        "    with open(jsonl_path, 'r') as f:\n",
        "        for line in f:\n",
        "            entry = json.loads(line)\n",
        "\n",
        "            features = np.array(entry['features'])  # shape: (n_segments, 100, 768)\n",
        "            label = entry['label']\n",
        "            sample_id = entry['id']\n",
        "\n",
        "            all_segments.append(features)\n",
        "            all_labels.append(label)\n",
        "            all_ids.append(sample_id)\n",
        "\n",
        "    return all_segments, all_labels, all_ids\n",
        "\n",
        "X_val_segments, y_val_segments, val_ids = load_validation_data(validation_data_path)\n",
        "# X_val_segments Shape of second sample: (20, 100, 768)\n",
        "\n",
        "X_val_segments = [np.asarray(segment).astype('float32') for segment in X_val_segments]\n",
        "y_val_segments = [np.float32(label) for label in y_val_segments]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee43b79",
      "metadata": {
        "id": "9ee43b79"
      },
      "outputs": [],
      "source": [
        "#====================================================\n",
        "# i don't know why it works. don't change. it is oldest version, but only it works\n",
        "#====================================================\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import namedtuple\n",
        "import math\n",
        "from pytorch_transformers.modeling_bert import (\n",
        "    BertEncoder,\n",
        "    BertPreTrainedModel,\n",
        "    BertConfig,\n",
        ")\n",
        "import os, torch\n",
        "from collections import namedtuple\n",
        "\n",
        "class GeLU(nn.Module):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "    For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class BertLayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\"\"\"\n",
        "        super(BertLayerNorm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.weight * x + self.bias\n",
        "\n",
        "\n",
        "class mlp_meta(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.hid_dim, config.hid_dim),\n",
        "            GeLU(),\n",
        "            BertLayerNorm(config.hid_dim, eps=1e-12),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "class Bert_Transformer_Layer(BertPreTrainedModel):\n",
        "    def __init__(self, fusion_config):\n",
        "        super().__init__(BertConfig(**fusion_config))\n",
        "        bertconfig_fusion = BertConfig(**fusion_config)\n",
        "        self.encoder = BertEncoder(bertconfig_fusion)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input, mask=None):\n",
        "        \"\"\"\n",
        "        input:(bs, 4, dim)\n",
        "        \"\"\"\n",
        "        batch, feats, dim= input.size() # was batch, feats, dim\n",
        "        if mask is not None:\n",
        "            mask_ = torch.ones(size=(batch, feats), device=mask.device)\n",
        "            mask_[:, 1:] = mask\n",
        "            mask_ = torch.bmm(\n",
        "                mask_.view(batch, 1, -1).transpose(1, 2), mask_.view(batch, 1, -1)\n",
        "            )\n",
        "            mask_ = mask_.unsqueeze(1)\n",
        "\n",
        "        else:\n",
        "            mask = torch.Tensor([1.0]).to(input.device)\n",
        "            mask_ = mask.repeat(batch, 1, feats, feats)\n",
        "\n",
        "        extend_mask = (1 - mask_) * -10000\n",
        "        assert not extend_mask.requires_grad\n",
        "        head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        enc_output = self.encoder(input, extend_mask, head_mask=head_mask)\n",
        "        output = enc_output[0]\n",
        "        all_attention = enc_output[1]\n",
        "\n",
        "        return output, all_attention\n",
        "\n",
        "\n",
        "class mmdPreModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config,\n",
        "        num_mlp=0,\n",
        "        transformer_flag=False,\n",
        "        num_hidden_layers=1,\n",
        "        mlp_flag=True,\n",
        "    ):\n",
        "        super(mmdPreModel, self).__init__()\n",
        "        self.num_mlp = num_mlp\n",
        "        self.transformer_flag = transformer_flag\n",
        "        self.mlp_flag = mlp_flag\n",
        "        token_num = config.token_num\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.in_dim, config.hid_dim),\n",
        "            GeLU(),\n",
        "            BertLayerNorm(config.hid_dim, eps=1e-12),\n",
        "            nn.Dropout(config.dropout),\n",
        "            # nn.Linear(config.hid_dim, config.out_dim),\n",
        "        )\n",
        "        self.fusion_config = {\n",
        "            \"hidden_size\": config.in_dim,\n",
        "            \"num_hidden_layers\": num_hidden_layers,\n",
        "            \"num_attention_heads\": 4,\n",
        "            \"output_attentions\": True,\n",
        "        }\n",
        "        if self.num_mlp > 0:\n",
        "            self.mlp2 = nn.ModuleList([mlp_meta(config) for _ in range(self.num_mlp)])\n",
        "        if self.transformer_flag:\n",
        "            self.transformer = Bert_Transformer_Layer(self.fusion_config)\n",
        "        self.feature = nn.Linear(config.hid_dim * token_num, config.out_dim)\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        input: [batch, token_num, hidden_size], output: [batch, token_num * config.out_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        if self.transformer_flag:\n",
        "            features, _ = self.transformer(features)\n",
        "        if self.mlp_flag:\n",
        "            features = self.mlp(features)\n",
        "\n",
        "        if self.num_mlp > 0:\n",
        "            # features = self.mlp2(features)\n",
        "            for _ in range(1):\n",
        "                for mlp in self.mlp2:\n",
        "                    features = mlp(features)\n",
        "\n",
        "        features = self.feature(features.view(features.shape[0], -1))\n",
        "        return features  # features.view(features.shape[0], -1)\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_checkpoint(path, map_location):\n",
        "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
        "        raise FileNotFoundError(f\"Checkpoint missing or empty: {path}\")\n",
        "    with open(path, \"rb\") as f:\n",
        "        return torch.load(f, map_location=map_location)\n",
        "\n",
        "def save_checkpoint(path, net, sigma, sigma0_u, ep):\n",
        "    payload = {\n",
        "        \"net\": net.state_dict(),\n",
        "        \"sigma\": sigma.detach().cpu(),\n",
        "        \"sigma0_u\": sigma0_u.detach().cpu(),\n",
        "        \"ep\": ep.detach().cpu(),\n",
        "    }\n",
        "    tmp = path + \".tmp\"\n",
        "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
        "    with open(tmp, \"wb\"):\n",
        "        torch.save(payload, tmp)\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "# TODO: replace with your real initial values / shapes if not scalars\n",
        "def default_init_params(device):\n",
        "    sigma    = torch.tensor(1.0, device=device)\n",
        "    sigma0_u = torch.tensor(1.0, device=device)\n",
        "    ep       = torch.tensor(1e-6, device=device)\n",
        "    return sigma, sigma0_u, ep\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c28b0b52",
      "metadata": {
        "id": "c28b0b52"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# MMD + NetLoader (drop-in)\n",
        "# =========================\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import namedtuple\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Your feature extractor class must exist/imported as mmdPreModel ---\n",
        "# from your_module import mmdPreModel\n",
        "\n",
        "# ---------------------------\n",
        "# NetLoader: make params learn\n",
        "# ---------------------------\n",
        "class NetLoader:\n",
        "    def __init__(self, checkpoint_filename= net_pt_path):\n",
        "        token_num, hidden_size = 100, 768\n",
        "        Config = namedtuple(\"Config\", [\"in_dim\", \"hid_dim\", \"dropout\", \"out_dim\", \"token_num\"])\n",
        "        config = Config(in_dim=hidden_size, token_num=token_num, hid_dim=512, dropout=0.2, out_dim=300)\n",
        "        self.config = config\n",
        "\n",
        "        self.net = mmdPreModel(config=config, num_mlp=0, transformer_flag=True, num_hidden_layers=1)\n",
        "\n",
        "        ckpt = torch.load(checkpoint_filename, map_location=\"cpu\")\n",
        "        self.net.load_state_dict(ckpt[\"net\"])   # load network weights first\n",
        "        self.net = self.net.to(DEVICE)\n",
        "\n",
        "        # Ensure sigma, sigma0_u, ep are learnable Parameters ON self.net\n",
        "        self._ensure_learnable_param(\"sigma\",    ckpt.get(\"sigma\",    torch.tensor(1.0)))\n",
        "        self._ensure_learnable_param(\"sigma0_u\", ckpt.get(\"sigma0_u\", torch.tensor(1.0)))\n",
        "        self._ensure_learnable_param(\"ep\",       ckpt.get(\"ep\",       torch.tensor(1e-6)))\n",
        "\n",
        "        # Optional mirrors (so code that referenced net_loader.sigma still works)\n",
        "        self.sigma    = self.net.sigma\n",
        "        self.sigma0_u = self.net.sigma0_u\n",
        "        self.ep       = self.net.ep\n",
        "\n",
        "        # For inference feature extraction\n",
        "        self.net.eval()\n",
        "\n",
        "    def _ensure_learnable_param(self, name: str, value):\n",
        "        t = torch.as_tensor(value, dtype=torch.float32, device=DEVICE).clone().detach()\n",
        "        # If already a Parameter, just copy into .data\n",
        "        if isinstance(self.net._parameters.get(name, None), nn.Parameter):\n",
        "            with torch.no_grad():\n",
        "                self.net._parameters[name].data.copy_(t)\n",
        "            self.net._parameters[name].requires_grad = True\n",
        "            return\n",
        "        # If exists as a buffer, remove it so we can register a Parameter\n",
        "        if name in self.net._buffers:\n",
        "            self.net._buffers.pop(name)\n",
        "        # Register as Parameter\n",
        "        self.net.register_parameter(name, nn.Parameter(t, requires_grad=True))\n",
        "\n",
        "# ------------------------\n",
        "# Differentiable utilities\n",
        "# ------------------------\n",
        "def Pdist2(x, y=None):\n",
        "    \"\"\"Pairwise squared distances between rows of x and y.\"\"\"\n",
        "    if y is None:\n",
        "        y = x\n",
        "    x2 = (x * x).sum(dim=1, keepdim=True)                 # (m,1)\n",
        "    y2 = (y * y).sum(dim=1, keepdim=True).transpose(0, 1) # (1,n)\n",
        "    d2 = x2 + y2 - 2.0 * (x @ y.transpose(0, 1))\n",
        "    return torch.clamp_min(d2, 0.0)                       # numeric floor, keeps grads\n",
        "\n",
        "def flexible_kernel(X, Y, X_org, Y_org, sigma, sigma0=0.1, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Flexible kernel as in MMDu:\n",
        "      K = (1-Îµ) * exp( - (Dxy/Ïƒ0)^L - Dxy_org/Ïƒ ) + Îµ * exp( - Dxy_org/Ïƒ )\n",
        "    All ops tensorized & differentiable wrt sigma, sigma0, epsilon.\n",
        "    \"\"\"\n",
        "    dtype, device = X.dtype, X.device\n",
        "    sigma   = torch.as_tensor(sigma,   dtype=dtype, device=device)\n",
        "    sigma0  = torch.as_tensor(sigma0,  dtype=dtype, device=device)\n",
        "    epsilon = torch.as_tensor(epsilon, dtype=dtype, device=device)\n",
        "\n",
        "    # keep positive & stable\n",
        "    sigma   = torch.clamp_min(sigma,  1e-12)\n",
        "    sigma0  = torch.clamp_min(sigma0, 1e-12)\n",
        "    epsilon = torch.clamp(epsilon, 1e-12, 1.0 - 1e-12)\n",
        "\n",
        "    Dxy     = Pdist2(X, Y)\n",
        "    Dxy_org = Pdist2(X_org, Y_org)\n",
        "\n",
        "    L = 1.0\n",
        "    term_main = torch.exp(- (Dxy / sigma0) ** L - (Dxy_org / sigma))\n",
        "    term_aux  = torch.exp(- (Dxy_org / sigma))\n",
        "    return (1.0 - epsilon) * term_main + epsilon * term_aux\n",
        "\n",
        "def MMD_Diff_Var(Kyy, Kzz, Kxy, Kxz, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Variance of the difference statistic MMD(X,Y) - MMD(X,Z).\n",
        "    Differentiable (uses clamp_min instead of branching + .item()).\n",
        "    \"\"\"\n",
        "    device, dtype = Kyy.device, Kyy.dtype\n",
        "    epsilon = torch.as_tensor(epsilon, dtype=dtype, device=device)\n",
        "    epsilon = torch.clamp_min(epsilon, 1e-12)\n",
        "\n",
        "    m = Kxy.shape[0]\n",
        "    n = Kyy.shape[0]\n",
        "    r = Kzz.shape[0]\n",
        "\n",
        "    Kyynd = Kyy - torch.diag(torch.diag(Kyy))\n",
        "    Kzznd = Kzz - torch.diag(torch.diag(Kzz))\n",
        "\n",
        "    u_yy = Kyynd.sum() / (n * (n - 1))\n",
        "    u_zz = Kzznd.sum() / (r * (r - 1))\n",
        "    u_xy = Kxy.sum()   / (m * n)\n",
        "    u_xz = Kxz.sum()   / (m * r)\n",
        "\n",
        "    t1 = (Kyynd.t() @ Kyynd).sum() / (n**3)      - u_yy**2\n",
        "    t2 = (Kxy.t()   @ Kxy   ).sum() / (n**2 * m) - u_xy**2\n",
        "    t3 = (Kxy       @ Kxy.t()).sum() / (n * m**2) - u_xy**2\n",
        "    t4 = (Kzznd.t() @ Kzznd).sum() / (r**3)      - u_zz**2\n",
        "    t5 = (Kxz       @ Kxz.t()).sum() / (r * m**2) - u_xz**2\n",
        "    t6 = (Kxz.t()   @ Kxz   ).sum() / (r**2 * m) - u_xz**2\n",
        "    t7 = (Kyynd     @ Kxy.t()).sum() / (n**2 * m) - u_yy * u_xy\n",
        "    t8 = (Kxy.t()   @ Kxz   ).sum() / (n * m * r) - u_xz * u_xy\n",
        "    t9 = (Kzznd     @ Kxz.t()).sum() / (r**2 * m) - u_zz * u_xz\n",
        "\n",
        "    zeta1 = torch.clamp_min(t1 + t2 + t3 + t4 + t5 + t6 - 2 * (t7 + t8 + t9), epsilon)\n",
        "    zeta2 = torch.clamp_min(\n",
        "        (1.0 / (m * (m - 1))) * ((Kyynd - Kzznd - Kxy.t() - Kxy + Kxz + Kxz.t()) ** 2).sum()\n",
        "        - (u_yy - 2.0 * u_xy - (u_zz - 2.0 * u_xz)) ** 2,\n",
        "        epsilon\n",
        "    )\n",
        "\n",
        "    Var    = (4.0 * (m - 2) / (m * (m - 1))) * zeta1\n",
        "    Var_z2 = Var + (2.0 / (m * (m - 1))) * zeta2\n",
        "\n",
        "    # lightweight debug (safe for printing only)\n",
        "    data = {\n",
        "        \"t1\": t1.detach().cpu().item(),\n",
        "        \"t2\": t2.detach().cpu().item(),\n",
        "        \"t3\": t3.detach().cpu().item(),\n",
        "        \"t4\": t4.detach().cpu().item(),\n",
        "        \"t5\": t5.detach().cpu().item(),\n",
        "        \"t6\": t6.detach().cpu().item(),\n",
        "        \"t7\": t7.detach().cpu().item(),\n",
        "        \"t8\": t8.detach().cpu().item(),\n",
        "        \"t9\": t9.detach().cpu().item(),\n",
        "        \"zeta1\": zeta1.detach().cpu().item(),\n",
        "        \"zeta2\": zeta2.detach().cpu().item(),\n",
        "    }\n",
        "    return Var, Var_z2, data\n",
        "\n",
        "# --------------------------------\n",
        "# 3-sample test (differentiable t)\n",
        "# --------------------------------\n",
        "def MMD_3_Sample_Test(\n",
        "    ref_fea, fea_y, fea_z,\n",
        "    ref_fea_org, fea_y_org, fea_z_org,\n",
        "    sigma, sigma0, epsilon, alpha,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns: h (int), p_value (tensor), t_std (tensor), t_raw (tensor), Diff_Var (tensor)\n",
        "    You can still call exactly as:\n",
        "      h, p_value, t, *rest = MMD_3_Sample_Test(..., net.sigma, net.sigma0_u, net.ep, 0.05)\n",
        "    \"\"\"\n",
        "    # Use features as-is; gradients will only flow into sigma/sigma0_u/epsilon\n",
        "    X, Y, Z = ref_fea, fea_y, fea_z\n",
        "    X_org, Y_org, Z_org = ref_fea_org, fea_y_org, fea_z_org\n",
        "\n",
        "    Kyy = flexible_kernel(Y, Y, Y_org, Y_org, sigma, sigma0, epsilon)\n",
        "    Kzz = flexible_kernel(Z, Z, Z_org, Z_org, sigma, sigma0, epsilon)\n",
        "    Kxy = flexible_kernel(X, Y, X_org, Y_org, sigma, sigma0, epsilon)\n",
        "    Kxz = flexible_kernel(X, Z, X_org, Z_org, sigma, sigma0, epsilon)\n",
        "\n",
        "    Kyynd = Kyy - torch.diag(torch.diag(Kyy))\n",
        "    Kzznd = Kzz - torch.diag(torch.diag(Kzz))\n",
        "\n",
        "    u_yy = Kyynd.sum() / (Y.shape[0] * (Y.shape[0] - 1))\n",
        "    u_zz = Kzznd.sum() / (Z.shape[0] * (Z.shape[0] - 1))\n",
        "    u_xy = Kxy.sum()   / (X.shape[0] * Y.shape[0])\n",
        "    u_xz = Kxz.sum()   / (X.shape[0] * Z.shape[0])\n",
        "\n",
        "    # Directional difference (good loss target)\n",
        "    t_raw = u_yy - 2.0 * u_xy - (u_zz - 2.0 * u_xz)\n",
        "\n",
        "    # Var for standardization; keep differentiable w.r.t epsilon\n",
        "    Diff_Var, _, _ = MMD_Diff_Var(Kyy, Kzz, Kxy, Kxz, epsilon)\n",
        "    Diff_Var = torch.clamp_min(Diff_Var, torch.as_tensor(epsilon, dtype=Diff_Var.dtype, device=Diff_Var.device))\n",
        "\n",
        "    t_std = t_raw / torch.sqrt(Diff_Var + 1e-12)\n",
        "\n",
        "    # Decision/p-value (no grad needed)\n",
        "    with torch.no_grad():\n",
        "        from torch.distributions.normal import Normal\n",
        "        # Two-sided by default; switch to one-sided if needed.\n",
        "        p_value = 2.0 * (1.0 - Normal(0, 1).cdf(torch.abs(t_std)))\n",
        "        h = (p_value <= alpha).to(torch.int32)\n",
        "        ##############################################\n",
        "        # if p_value <= alpha:\n",
        "        #     h = 1\n",
        "        # else:\n",
        "        #     h = 0\n",
        "        ###############################################\n",
        "    return h, p_value, t_std, t_raw, Diff_Var\n",
        "\n",
        "# ==========================================\n",
        "# Example: one-step MMD update of kernel pars\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def _flatten(x):  # helper to build X_org/Y_org/Z_org\n",
        "    return x.view(x.size(0), -1)\n",
        "\n",
        "def mmd_update_step(net_loader,\n",
        "                    feature_for_sents_sample,     # (B, T, D)\n",
        "                    feature_mgt_ref_sample,       # (B, T, D)\n",
        "                    feature_hwt_ref_sample,       # (B, T, D)\n",
        "                    lr=0.2, two_sided=False, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Runs one gradient step that updates net.sigma, net.sigma0_u, net.ep using the MMD statistic.\n",
        "    Your call pattern stays the same inside the MMD function.\n",
        "    \"\"\"\n",
        "    net = net_loader  # so you can refer to net.sigma etc.\n",
        "    # Build a tiny optimizer on the three kernel params\n",
        "    opt_kernel = torch.optim.Adam([net.sigma, net.sigma0_u, net.ep], lr=lr)\n",
        "\n",
        "    # 1) Extract learned features (freeze backbone)\n",
        "    with torch.no_grad():\n",
        "        Xl = net.net(feature_for_sents_sample.to(DEVICE))\n",
        "        Yl = net.net(feature_mgt_ref_sample.to(DEVICE))\n",
        "        Zl = net.net(feature_hwt_ref_sample.to(DEVICE))\n",
        "\n",
        "    # 2) Originals flattened (no grad path to backbone needed)\n",
        "    Xf = _flatten(feature_for_sents_sample.to(DEVICE))\n",
        "    Yf = _flatten(feature_mgt_ref_sample.to(DEVICE))\n",
        "    Zf = _flatten(feature_hwt_ref_sample.to(DEVICE))\n",
        "\n",
        "    # 3) Compute MMD test and backprop through t_raw (or t_std)\n",
        "    opt_kernel.zero_grad()\n",
        "    h_u, p_value, t_std, t_raw, diff_var = MMD_3_Sample_Test(\n",
        "        Xl, Yl, Zl,\n",
        "        Xf, Yf, Zf,\n",
        "        net.sigma, net.sigma0_u, net.ep,\n",
        "        alpha\n",
        "    )\n",
        "    # Choose objective: directional or magnitude\n",
        "    loss = -t_raw if not two_sided else -t_raw.abs()\n",
        "    loss.backward()\n",
        "    # Optional: inspect gradients once\n",
        "    # print(\"grads:\", net.sigma.grad, net.sigma0_u.grad, net.ep.grad)\n",
        "\n",
        "    opt_kernel.step()\n",
        "\n",
        "    # Keep params in sane ranges (optional but recommended)\n",
        "    with torch.no_grad():\n",
        "        net.sigma.clamp_(1e-8, 1e6)\n",
        "        net.sigma0_u.clamp_(1e-8, 1e6)\n",
        "        net.ep.clamp_(1e-12, 1e-2)\n",
        "\n",
        "    # Return tensors for logging or further use\n",
        "    return {\n",
        "        \"h\": int(h_u.item()),\n",
        "        \"p_value\": float(p_value.item()),\n",
        "        \"t_std\": float(t_std.detach().cpu().item()),\n",
        "        \"t_raw\": float(t_raw.detach().cpu().item()),\n",
        "        \"diff_var\": float(diff_var.detach().cpu().item()),\n",
        "        \"sigma\": float(net.sigma.detach().cpu().item()),\n",
        "        \"sigma0_u\": float(net.sigma0_u.detach().cpu().item()),\n",
        "        \"ep\": float(net.ep.detach().cpu().item()),\n",
        "    }\n",
        "\n",
        "# ==========================\n",
        "# (Optional) quick sanity IO\n",
        "# ==========================\n",
        "def save_updated_checkpoint(net_loader, path=\"net.pt\"):\n",
        "    \"\"\"Persist updated net + kernel params back to disk.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        torch.save({\n",
        "            \"net\": net_loader.net.state_dict(),\n",
        "            \"sigma\":    net_loader.sigma.detach().cpu(),\n",
        "            \"sigma0_u\": net_loader.sigma0_u.detach().cpu(),\n",
        "            \"ep\":       net_loader.ep.detach().cpu(),\n",
        "        }, path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84427a2d",
      "metadata": {
        "id": "84427a2d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# If you already define DEVICE elsewhere, you can remove this.\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---- helpers ---------------------------------------------------------------\n",
        "\n",
        "def to_tensor(x, device=None, dtype=torch.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x.to(device=device, dtype=dtype, non_blocking=True)\n",
        "    return torch.as_tensor(x, dtype=dtype, device=device)\n",
        "\n",
        "def feature_ref_loader(load_ref_data, num_ref=5000):\n",
        "    \"\"\"\n",
        "    Torch-only sampler (no NumPy). Keeps gradients/layouts intact.\n",
        "    \"\"\"\n",
        "    t = load_ref_data\n",
        "    if num_ref is not None and num_ref > 0 and t.shape[0] > num_ref:\n",
        "        idx = torch.randperm(t.shape[0], device=t.device)[:num_ref]\n",
        "        t = t[idx]\n",
        "    return t\n",
        "\n",
        "def _ensure_param(obj, name, value, device):\n",
        "    \"\"\"\n",
        "    Make sure obj.<name> is an nn.Parameter on the correct device/dtype,\n",
        "    and copy 'value' into it without losing Parameter-ness.\n",
        "    \"\"\"\n",
        "    val = torch.as_tensor(value, dtype=torch.float32, device=device).clone().detach()\n",
        "    p = getattr(obj, name, None)\n",
        "    if isinstance(p, nn.Parameter):\n",
        "        with torch.no_grad():\n",
        "            p.data.copy_(val)\n",
        "        p.requires_grad_(True)\n",
        "    else:\n",
        "        setattr(obj, name, nn.Parameter(val, requires_grad=True))\n",
        "    return getattr(obj, name)\n",
        "\n",
        "# ---- your class ------------------------------------------------------------\n",
        "\n",
        "class RelativeTester:\n",
        "    def __init__(self, feature_hwt_ref, feature_mgt_ref, feature_test, *,\n",
        "                 device=DEVICE, dtype=torch.float32, max_ref=8161, net_loader_cls=None): # was defult 5000 which is not correct; should be 8161\n",
        "        \"\"\"\n",
        "        Minimal-change init:\n",
        "        - convert once\n",
        "        - sample refs with torch.randperm (no NumPy)\n",
        "        - keep tensors contiguous\n",
        "        - build NetLoader once (optionally injected)\n",
        "        \"\"\"\n",
        "        print(\"Relative Tester init\")\n",
        "        self.device, self.dtype = device, dtype\n",
        "\n",
        "        # Convert inputs once\n",
        "        feature_hwt_ref = to_tensor(feature_hwt_ref, device, dtype)\n",
        "        feature_mgt_ref = to_tensor(feature_mgt_ref, device, dtype)\n",
        "        feature_test    = to_tensor(feature_test,    device, dtype)\n",
        "\n",
        "        # Optional subsample to control memory (kept disabled by default)\n",
        "        feature_hwt_ref = feature_ref_loader(feature_hwt_ref, num_ref=max_ref) # load all samples\n",
        "        feature_mgt_ref = feature_ref_loader(feature_mgt_ref, num_ref=max_ref)\n",
        "\n",
        "        self.feature_hwt_ref = feature_hwt_ref.contiguous()\n",
        "        self.feature_mgt_ref = feature_mgt_ref.contiguous()\n",
        "        self.feature_test    = feature_test.contiguous()\n",
        "\n",
        "        # Build/load net once here (was inside test()) to avoid reloading each call\n",
        "        # If you must keep the old behavior, you can move this back into test().\n",
        "        NetLoaderCls = NetLoader if net_loader_cls is None else net_loader_cls\n",
        "        self.net = NetLoaderCls()  # assumes your NetLoader already loads 'net.pt'\n",
        "        self.net.net.eval()        # deterministic inference for features\n",
        "\n",
        "        # --- Ensure kernel scalars are learnable Parameters on self.net (critical) ---\n",
        "        # If NetLoader already registers them as nn.Parameter, these lines simply copy.\n",
        "        self.net.sigma    = _ensure_param(self.net, \"sigma\",    getattr(self.net, \"sigma\",    1.0),   self.device)\n",
        "        self.net.sigma0_u = _ensure_param(self.net, \"sigma0_u\", getattr(self.net, \"sigma0_u\", 1.0),   self.device)\n",
        "        self.net.ep       = _ensure_param(self.net, \"ep\",       getattr(self.net, \"ep\",       1e-6),  self.device)\n",
        "\n",
        "        # A tiny optimizer only for the three kernel params (built once)\n",
        "        # kernel hyperparameters (Ïƒ,Ïƒ0â€‹,Îµ) to maximize the three-sample MMD test statistic ð‘¡\n",
        "\n",
        "        self.opt_kernel = torch.optim.Adam([self.net.sigma, self.net.sigma0_u, self.net.ep], lr=0.2)\n",
        "\n",
        "    def test(self, threshold=0.2, rounds=800, two_sided=False, update_kernel=True):  # two_sided = True 0.77\n",
        "        \"\"\"\n",
        "        Runs rounds of the 3-sample test; optionally updates sigma/sigma0_u/ep\n",
        "        via the MMD statistic each round (minimal change to your flow).\n",
        "\n",
        "        Returns the same \"probability\" as your code: abs(power - 1.0).\n",
        "        \"\"\"\n",
        "        # Safety checks\n",
        "        min_len = min(len(self.feature_test), len(self.feature_hwt_ref), len(self.feature_mgt_ref))\n",
        "        assert min_len > 0, \"Empty inputs to RelativeTester.test()\"\n",
        "\n",
        "        h_u_list, p_value_list, t_list = [], [], []\n",
        "\n",
        "        for _ in range(rounds):\n",
        "            # Sample equal sized mini-batches\n",
        "            ix = torch.randperm(len(self.feature_test),     device=self.device)[:min_len]\n",
        "            iy = torch.randperm(len(self.feature_hwt_ref),  device=self.device)[:min_len]\n",
        "            iz = torch.randperm(len(self.feature_mgt_ref),  device=self.device)[:min_len]\n",
        "\n",
        "            feature_for_sents_sample = self.feature_test[ix]\n",
        "            feature_hwt_ref_sample   = self.feature_hwt_ref[iy]\n",
        "            feature_mgt_ref_sample   = self.feature_mgt_ref[iz]\n",
        "\n",
        "            # Freeze backbone; just compute learned features\n",
        "            with torch.no_grad():\n",
        "                Xl = self.net.net(feature_for_sents_sample)  # [B, T*out_dim] (per your comment)\n",
        "                Yl = self.net.net(feature_mgt_ref_sample)\n",
        "                Zl = self.net.net(feature_hwt_ref_sample)    # (you noted swapped order)\n",
        "\n",
        "            # Flatten originals once (no need for torch.no_grad() here)\n",
        "            Xf = feature_for_sents_sample.view(feature_for_sents_sample.size(0), -1)\n",
        "            Yf = feature_mgt_ref_sample.view(feature_mgt_ref_sample.size(0), -1)\n",
        "            Zf = feature_hwt_ref_sample.view(feature_hwt_ref_sample.size(0), -1)\n",
        "\n",
        "            # --- Call your MMD test exactly as before ---\n",
        "            h_u, p_value, t, *rest = MMD_3_Sample_Test(\n",
        "                Xl, Yl, Zl,   # learned features\n",
        "                Xf, Yf, Zf,   # original/flattened features\n",
        "                self.net.sigma, self.net.sigma0_u, self.net.ep,\n",
        "                0.05, # was 0.05, AUROC converged to 0.96 after  600 rounds\n",
        "            )\n",
        "\n",
        "            # --- Optional kernel-parameter update via MMD ---\n",
        "            # If your MMD returns tensors (recommended), we can backprop through 't' (or t_raw in rest)\n",
        "            if update_kernel:\n",
        "                # Prefer the raw directional statistic if you returned it in *rest*\n",
        "                t_raw = None\n",
        "                if rest:\n",
        "                    # Heuristic: pick the first tensor scalar in *rest* as t_raw if present\n",
        "                    for r in rest:\n",
        "                        if torch.is_tensor(r) and r.numel() == 1:\n",
        "                            t_raw = r\n",
        "                            break\n",
        "                target = t_raw if (t_raw is not None and torch.is_tensor(t_raw)) else (t if torch.is_tensor(t) else None)\n",
        "\n",
        "                if target is not None:\n",
        "                    self.opt_kernel.zero_grad()\n",
        "                    loss = -target.abs() if two_sided else -target\n",
        "                    loss.backward()\n",
        "                    # grads should be finite; if you want, print once:\n",
        "                    # print(self.net.sigma.grad, self.net.sigma0_u.grad, self.net.ep.grad)\n",
        "                    self.opt_kernel.step()\n",
        "\n",
        "                    # keep params positive and in sane ranges\n",
        "                    with torch.no_grad():\n",
        "                        self.net.sigma.clamp_(1e-8, 1e6)\n",
        "                        self.net.sigma0_u.clamp_(1e-8, 1e6)\n",
        "                        self.net.ep.clamp_(1e-12, 1e-2)\n",
        "                # else: your MMD likely returned floats via .item(); then no gradient path exists.\n",
        "\n",
        "            # Loggers (cast to Python numbers for storage)\n",
        "            h_u_list.append(float(h_u))\n",
        "            p_value_list.append(float(p_value) if not torch.is_tensor(p_value) else float(p_value.item()))\n",
        "            t_list.append(float(t) if not torch.is_tensor(t) else float(t.item()))\n",
        "\n",
        "        power = sum(h_u_list) / len(h_u_list)\n",
        "        # Your original return: probability = abs(power - 1.0)\n",
        "        return abs(power - 1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de059a4b",
      "metadata": {
        "id": "de059a4b"
      },
      "outputs": [],
      "source": [
        "lis = []\n",
        "\n",
        "for i in X_val_segments:\n",
        "    nmb = i.shape[0]\n",
        "    lis.append(nmb)\n",
        "lis.sort()\n",
        "lis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa376fc",
      "metadata": {
        "id": "3fa376fc"
      },
      "outputs": [],
      "source": [
        "\"main\"\n",
        "\n",
        "prob_result_paragraph = []\n",
        "for segments in X_val_segments[:20]:\n",
        "    relative_tester = RelativeTester(HWT_ref,MGT_ref,segments) #feature_hwt_ref,feature_mgt_ref,feature_test\n",
        "    prob = relative_tester.test()\n",
        "    prob_result_paragraph.append(prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob_result_paragraph"
      ],
      "metadata": {
        "id": "q9TmHyIAnvXx"
      },
      "id": "q9TmHyIAnvXx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_result_paragraph # alpah = 0.027 0.096"
      ],
      "metadata": {
        "id": "_rD4JJeDo6M4"
      },
      "id": "_rD4JJeDo6M4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_result_paragraph # alhap == 0.025 result 0.096  , 0.05 - > 0.96; 0.02 -> 0.96"
      ],
      "metadata": {
        "id": "8FRLeRB8mOQM"
      },
      "id": "8FRLeRB8mOQM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_result_paragraph #round == 800 alhap == 0.015 result  0.95, which means there could be a summit  in the range of 0.02 to 0.01"
      ],
      "metadata": {
        "id": "_CobIdKIka9O"
      },
      "id": "_CobIdKIka9O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_result_paragraph #round == 800 alhap == 0.01 result  0.94, which means there could be a summit  in the range of 0.02 to 0.01"
      ],
      "metadata": {
        "id": "N1EEbi3HhKaR"
      },
      "id": "N1EEbi3HhKaR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_result_paragraph #round == 800 alhap == 0.009 result  0.93, which means there could be a summit  in the range of 0.02 to 0.009"
      ],
      "metadata": {
        "id": "cFF3eVAmfYiT"
      },
      "id": "cFF3eVAmfYiT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_result_paragraph #round == 800 alhap == 0.02 result 0.96 go into large"
      ],
      "metadata": {
        "id": "ill5rbHId5gx"
      },
      "id": "ill5rbHId5gx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_result_paragraph #round == 800 alhap == 0.09 result 0.94 go into small"
      ],
      "metadata": {
        "id": "SMZamfcVaEwG"
      },
      "id": "SMZamfcVaEwG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_result_paragraph #round == 800 alhap == 0.5 result 0.96 which means the prob have cnoveraged"
      ],
      "metadata": {
        "id": "DZfHn908Rl3s"
      },
      "id": "DZfHn908Rl3s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43fd0d83",
      "metadata": {
        "id": "43fd0d83"
      },
      "outputs": [],
      "source": [
        "prob_result_paragraph # round == 500 alpha = 0.05 result 0.96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a3deb5",
      "metadata": {
        "id": "87a3deb5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "data = torch.load(net_pt_path, map_location=DEVICE)\n",
        "print(type(data)) #dict_keys(['net', 'sigma', 'sigma0_u', 'ep'])\n",
        "print(data.keys())\n",
        "print(data['sigma']) #tensor(1.)\n",
        "print(data['sigma0_u'])#tensor(1.)\n",
        "print(data['ep'])#tensor(1.0000e-06)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4491a214",
      "metadata": {
        "id": "4491a214"
      },
      "outputs": [],
      "source": [
        "y_val_segments[:20]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_segments[17].shape"
      ],
      "metadata": {
        "id": "xA9KfF7_F2Vs"
      },
      "id": "xA9KfF7_F2Vs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3084516",
      "metadata": {
        "id": "d3084516"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# 1) Replace with your data\n",
        "# y_true: ground-truth labels (0/1)\n",
        "# y_score: predicted scores/probabilities for the positive class\n",
        "y_true  = y_val_segments          # <- true labels\n",
        "y_score = prob_result_paragraph  # <- model scores\n",
        "\n",
        "# 2) Basic checks\n",
        "y_true = np.asarray(y_true, dtype=int)\n",
        "y_score = np.asarray(y_score, dtype=float)\n",
        "assert y_true.shape[0] == y_score.shape[0], \"y_true and y_score must have same length\"\n",
        "assert set(np.unique(y_true)).issubset({0, 1}), \"y_true must be binary (0/1)\"\n",
        "\n",
        "# 3) Compute ROC and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "auc = roc_auc_score(y_true, y_score)\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "# 4) Plot ROC curve\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, label=f\"ROC (AUC = {auc:.3f})\", linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)  # diagonal chance line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc4de77",
      "metadata": {
        "id": "6dc4de77"
      },
      "outputs": [],
      "source": [
        "# ------------------ Predict on test ------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c665c3af",
      "metadata": {
        "id": "c665c3af"
      },
      "outputs": [],
      "source": [
        "def load_jsonl_test(path):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          paragraphs: list of (n_segments,100,768) arrays (float32)\n",
        "          ids:        list of str/int\n",
        "        \"\"\"\n",
        "        paragraphs, ids = [], []\n",
        "        with open(path, \"r\") as f:\n",
        "            for line in f:\n",
        "                row = json.loads(line)\n",
        "                paragraphs.append(np.asarray(row[\"features\"], dtype=\"float32\"))\n",
        "                ids.append(row[\"id\"])\n",
        "        return paragraphs, ids\n",
        "\n",
        "test_path = '/content/drive/MyDrive/test_features.jsonl'\n",
        "X_test_segments, test_ids = load_jsonl_test(test_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46a28bed",
      "metadata": {
        "id": "46a28bed"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "prob_result_paragraph = []\n",
        "for segments in X_test_segments:\n",
        "    relative_tester = RelativeTester(HWT_ref,MGT_ref,segments) #feature_hwt_ref,feature_mgt_ref,feature_test\n",
        "    prob = relative_tester.test()\n",
        "    prob_result_paragraph.append(prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b66c15f",
      "metadata": {
        "id": "6b66c15f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# ------------------ Package submission ------------------\n",
        "df_pred = pd.DataFrame({\"id\": test_ids, \"y_prob\": prob_result_paragraph})\n",
        "out_csv = \"submission_fucking_giveup.csv\"\n",
        "df_pred.to_csv(out_csv, index=False)\n",
        "print(f\"[done] Wrote {out_csv}\")\n",
        "print(df_pred.head())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PyTorch (torch)",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}